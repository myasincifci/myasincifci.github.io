<!doctype html><html class="no-js"><head><meta charset="utf-8"><title>3DGS</title><meta name="description" content=""><meta name="viewport" content="width=device-width">
<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
<link href="http://fonts.googleapis.com/css?family=Raleway:300,400,600" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="style.css">
        <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <!--<link rel="stylesheet" href="styles/main.37ab405b.css">-->
<body>
<!--[if lt IE 7]>
<p class="browsehappy">You are using an 
    <strong>outdated</strong> browser. Please 
    <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.
</p>
<![endif]-->
<div class="container">

    <nav class="navbar">
        <div class="container">
            <ul class="navbar-list">
                <li class="navbar-item">
                    <a class="navbar-link" href="#intro">Intro</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#theory">Theory</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#implementation">Implementation</a>
                </li>
                <li class="navbar-item">
                    <a class="navbar-link" href="#results">Results</a>
                </li>
                <!-- <li class="navbar-item">
                    <a class="navbar-link" href="#code">Code</a>
                </li> -->
                <!-- <li class="navbar-item">
                    <a class="navbar-link" href="#math">Math</a>
                </li> -->
                <li class="navbar-item">
                    <a class="navbar-link" href="#references">References</a>
                </li>
            </ul>
        </div>
    </nav>

    <section class="header" id="intro">
        <h2 class="title">3D Gaussian Splatting</h2>
        <h6>Project by Mehmet Yasin Cifci (<a href="mailto:cifci@campus.tu-berlin.de">cifci@campus.tu-berlin.de</a>)
        </h6>

        <div class="row">
<!--            <div class="one column category" style="text-align: center;">-->
<!--                <h5 class="docs-header">Teaser Image</h5>-->
                <img class="u-max-full-width" src="images/training.gif">
                <p>
                    In Gaussian Splatting we carefully arrange a number of 
                    multi-variate Gaussians in space such that they represent 
                    a scene. Each Gaussian is assigned a color and can be rendered 
                    by evaluating it's probability density function. By carefully 
                    tuning each Gaussian's parameters to reconstruct ground truth 
                    views of the scene, it is possible to represent arbitrary scenes 
                    like the one in the two-dimensional example above. Extending this 
                    to the third dimension is particularly interesting because it 
                    allows us to render the scene from views that don't exist in the 
                    ground truth, allowing us to synthesize novel views.
                    This idea is introduced in the paper "3D Gaussian Splatting for 
                    Real-Time Radiance Field Rendering" <a href="#1">[1]</a> which we 
                    have reimplemented using the PyTorch library. The following will
                    give an overview of the theory and implementation details.
                </p>
            </div>
    </section>

    <div class="docs-section" id="theory">
        <h3 class="section-heading">Theory</h3>
        <p class="section-description">
            At the center of 3d Gaussian splatting lay two components:
            
            <ul>
                <li>
                    A set of <b>Gaussians</b> that are each described by an array of 
                    parameters that specify their position, shape, color, and 
                    opacity. These can be initialized randomly or using a point 
                    cloud generated by photogrammetric techniques like structure 
                    from motion <a href="#1">[2]</a>.
                </li>
                <li>
                    A set of <b>cameras/views</b> of the scene that we would like to 
                    reconstruct. This includes the ground truth image that has 
                    been recorded by the respective camera as well as the camera's
                    intrinsic and extrinsic information.
                </li>
            </ul>

             
            Given these two, the objective is to optimize the values of 
            the Gaussians' parameters s.t. they are maximally similar to 
            the ground truth images when rendered by each of the cameras.
        </p>

        <h5>Optimization Procedure</h5>

        <div class="row">
            <div class="column category" style="text-align: center;">
                <h5 id=figure1 class="docs-header">Training Pipeline</h5>
                <img class="u-max-full-width" src="images/gaussian_splatting_pipeline.png">
                    <p>Figure 1: Overview of the training pipeline as it is described in <a href="#1">[1]</a>.
                    </p>
            </div>
        </div>

        <p>
            Figure 1 illustrates the optimization procedure 
            step by step:
            <ol>
                <li>
                    One of the cameras is randomly selected and the Gaussians
                    are projected onto it's uv-plane.
                </li>
                <li>
                    The scene is rendered by the differentiable rasterizer 
                    which produces an image.
                </li>
                <li>
                    The loss between the rendered image and the camera's ground truth is
                    backpropagated trough the computational graph to update the gaussians
                    parameters using gradient descent.
                </li>
            </ol>
            This procedure is repeated until the overall loss is minimized which results
            in the gaussians representing the original scene. 
        </p>


        <p>
            Note that the training procedure
            shown in figure 1 also includes an "Adaptive Density Control" component that
            regulates the amount of gaussians in the scene dynamically. For the sake of simplicity, 
            we haven't implemented this component since it is not integral to the algorithm.
        </p>
        

        <h5>Formalization of Gaussians</h5>
        <p>
            Before diving deeper into the theorethical details, we will first formalize how we are 
            representing Gaussians: A Gaussian can be evaluated at a coordinate \(\mathbf{x}\) using 
            it's probability density function: 

            \[G(\pmb{x}) = N e^{- \frac{1}{2}(\pmb{x} - \pmb{\mu})^T \pmb{\Sigma}^{-1}(\pmb{x} - \pmb{\mu})}\]

            with mean:
            
            \[\pmb{\mu} = (\mu_x,\mu_y,\mu_z)^T,~\text{where}~ x,y,z \in \mathbb{R}\]
            
            covariance matrix:
            
            \[\pmb{\Sigma} \in \mathbb{R}^{3 \times 3}\]

            and normalization factor \(N\).

        </p>
        <p>
            Because the covariance matrix must be positive semi-definite, this property needs to be 
            upheld during the optimization process. To achieve this it is decomposed into a rotation 
            matrix \(\mathbf{R}\) and a scaling matrix \(\mathbf{S}\) which can be optimzed 
            independently without a constraint and from which a positive semi-definite covariance 
            matrix can be reconstructed:

            \[\pmb{\Sigma} = \mathbf{R}\mathbf{S}\mathbf{S}^T\mathbf{R}^T.\]

            \(\mathbf{S}\) can be condensed into \(\mathbf{s}=(s_x, s_y, s_z)^T\) and likewise 
            \(\mathbf{R}\) can be represented by a quaternion \(\mathbf{s}=(q_x, q_y, q_z, q_w)^T\).
        </p>
        <p>
            Next, we also need to assign a color value to the Gaussian. In the reference 
            implementation this is realized using spherical harmonics. This allows to not only 
            represent a static color value but a viewing angle dependent color. However for the sake 
            of simplicity we will model the color using a single RGB color value:
            
            \[\mathbf{c} = (r,g,b),~\text{where}~ r,g,b \in [0,1].\]

            Finally we note that because of the normalization constant \(N\), 
            the opacity of the Gaussian will change with it's covariance matrix. 
            To disentangle the opacity from the covariance we omit the 
            normalization and instead replace it with an opacity parameter:

            \[o \in [0,1].\]

            As a result we define the following representation of a Gaussian:

            \[\mathbf{g} = (\pmb{\mu}, \mathbf{s}, \mathbf{q}, \mathbf{c}, o)^T \in \mathbb{R}^{14}\]

            (Note: \(\mathbf{c}\) and \(o\) are clipped into the correct range during rendering.)
        </p>

        <h5>Rendering</h5>
            <p>
                This section goes over the render process that consists of the projection and rasterization
                steps:
            </p>    
        
            <h6>Projection</h6>
            <p>
                Let \(\mathbf{K}\) be the pinhole projection matrix. Then the projection of
                the mean is computed as
            
                \[\pmb{\mu}_{\text{2D}} = \begin{bmatrix}x'/z' \\ y'/z'\end{bmatrix} , \begin{bmatrix} \mu_x' \\ \mu_y'\\ \mu_z'\end{bmatrix}= \mathbf K \begin{bmatrix}\mu_x \\ \mu_y\\ \mu_z\end{bmatrix}.\]

                <div class="u-pull-right" style="text-align: center; width:300px; margin-left:20px">
                    <h5 id="figure2" class="docs-header">Figure 2</h5>
                    <img class="u-max-full-width" src="images/ewa_splatting.png">
                    <p>Applying \(\mathbf{K}\) directly (Figure from <a href="#3">[3]</a>)</p>
                </div>

                Since the pinhole projection is not an affine transformation, the result of
                applying it directly to the 3D-Gaussian will not result in a 2D-Gaussian 
                (as illustrated in <a href="#figure2">Figure 2</a>). 

                Therefore we are performing an affine approximation at the respective Gaussian's 
                mean with:

                \[ \pmb{\Sigma}_{\text{2D}} = \mathbf{J}\pmb{\Sigma}\mathbf{J}^T \]

                where 

                \[ \mathbf{J}(\pmb{\mu}) = \begin{bmatrix}
                \frac{f_x}{\mu_z} & 0 & -\frac{f_x\mu_x}{\mu_z^2} \\
                0 & \frac{f_y}{\mu_z} & -\frac{f_y\mu_y}{\mu_z^2} \\
                \end{bmatrix}
                \]

                and \(\mathbf{f} = (f_x, f_y)^T\) is the cameras focal point (as described in <a href="#3">[3]</a>). <br><br>
                Note: The calculations above assume that the camera is positioned at the origin.
                Otherwise both transformations \(\mathbf{K}\) and \(\mathbf{J}\) must be right 
                hand multiplied with the world-to-camera transform.
            </p>

            <h6>Rasterization</h6>
            Once the Gaussians have been projected to the uv-plane, they can be rasterized by 
            performing alpha blending. This is done with the following formula:

            \[
                C(\mathbf{x})=\sum_{i \in \mathcal{N}} c_i \alpha_i \prod_{j=1}^{i-1}\left(1-\alpha_j\right).
            \]

            It states that the color value \(C\) at a pixel coordinate \(\mathbf{x}\) is computed 
            by blending each Gaussians color weighted by it's alpha value iterating from the closest
            to the farthest Gaussian in the set of depth-sorted Gaussians \(\mathcal{N}\).
            The overall transparency \(\alpha_i\) is computed as 

            \[ \alpha_i = o_i G(x).\]

            <h6>Tile Based Rendering</h6>
            <p>
                Because the density of a Gaussian outside of a certain range will be very close to zero,
                it's contribution to the color value of a pixel will be negligible if the pixel's
                coordinate is outside of that range. With tile based rendering we are exploiting this 
                property by only including these Gaussians into the computation of a pixels color value
                that are within approximately three times the standard deviation of that Gaussian.
            </p>

            <p>
                To achieve this, the canvas is first partitioned into square tiles. An oriented bounding 
                box is computed to encapsulate every Gaussian and the Gaussians are sorted into the 
                tiles where a Gaussian is assigned to each tile that it's bounding box intersects.
                Then the pixels color values can be computed on a per tile basis only for the assigned 
                Gaussians, significantly reducing computational complexity. 
            </p>

            <h6>Computing the Bounding-Box</h6>
            Given a 2x2 Covariance Matrix

            \[\mathbf{A} = \begin{bmatrix}
                            a & b\\
                            b & c
                            \end{bmatrix}\]

            it's eigenvalues can be computed as 

            \[
                \lambda_1, \lambda_2 = \frac{1}{2} (a + d\pm\sqrt{  a^2 - 2ad + 4b^2 + d^2}).
            \]

            and the orientation can be computed as

            \[
                \theta = \begin{cases} 
                            0 & b = 0 \land a \geq c \\
                            \frac{\pi}{2} & b = 0 \land a \lt c \\
                            atan2(\lambda_1 - a, b) & \text{else} 
                        \end{cases}
            \]

            from which we construct the rotation matrix

            \[\mathbf{R} = \begin{bmatrix}
                            cos(\theta) & -sin(\theta)\\
                            sin(\theta) & cos(\theta)
                            \end{bmatrix}.\]

            Then, we construct a rectangle with the radii \(r_1 = \sqrt{\lambda_1}\) and \(r_2 = \sqrt{\lambda_2}\)
            and rotate it using the rotation matrix to get the final bounding-box.

    <div class="docs-section" id="implementation">
        <h3 class="section-heading">Implementation</h3>
        <p class="section-description">The following section gives an overview of our implementation of the 3DGS algorithm.</p>

        <p>
            We have chosen to implemented the complete algorithm in PyTorch
            because of the following features it provides:
            
            <ul>
                <li>
                    All standard PyTorch operations have their gradient computation 
                    preimplemented and using its autograd feature, any calculations 
                    gradient can be obtained automatically if the calculation is 
                    composed of these standard operations.
                </li>
                <li>
                    PyTorch provides features like broadcasting that allow 
                    vectorization of computations to execute them in parallel on 
                    the GPU.
                </li>
                <li>
                    We can make use of PyTorch's training API.
                </li>
            </ul>
             
            <p>
                We have implemented a Renderer as a torch module by inheriting 
                from nn.Module and divided the forward pass into three main 
                functons:
            </p>
            
            <p>
                <code>_project_gaussians</code>: Projects the 3D-Gaussians onto a 
                given camera's uv-plane and also performs a global sorting 
                operation according to the Gaussians' depth. 
            <p>
                <code>_tile_gaussians</code>: Computes bounding-boxes and assigns them 
                to tiles according to them. Produces a tile-map which contains 
                the gaussians assigned to each tile.
            </p> 

            <p>
                <code>_rasterize_tile_fast</code>: Computes color values for all 
                pixels of a fiven tile in a vectorized fashion. This function is 
                called sequentially for each tile. 
            </p> 

            <p>
                <p>
                All operations can be implemented following the calculation in the
                <a href="#theory">theory section</a> 
                using standard pytorch operations like torch.matmul, 
                torch.mul, torch.sort, and others which makes the rendering
                module automatically differentiable. In the next step we have
                implemented the training pipeline shown in <a href="#figure1">Figure 1</a> using 
                PyTorch's training API where we can instanciate and call the 
                renderer as a standard Pytorch module.
                </p>
            </p>
        </p>

        <h5>Limitations</h5>
        <p>
            A limiting factor of our implementation is that we are only 
            vectorizing the rasterization on a per-tile basis and sequentially
            call <b>_rasterize_tile_fast</b> on each tile. This is due to the 
            fact that there is a varying amount of gaussians assigned to each 
            tile which results in a jagged array for the tile-map. To our 
            knowleadge PyTorch does not support vectorization of jagged arrays 
            via torch.vmap or other ways, which forces us to perform the 
            computation sequentially. This leads to a significantly lower 
            execution speed compared to the reference implementation.
        </p>

    <div class="docs-section" id="results">
        <h3 class="section-heading">Results</h3>
        <p class="section-description">
            In this section we showcase some of the experiments and their results:
        </p>

        <h5>Experiments</h5>
        <p>
            To evaluate our implementation, we are runnining experiments on some of 
            the NeRF Synthetic Datasets <a href="#4">[4]</a>. All experiments are 
            perfomed at a resolution of 512 x 512 pixels with 5,000 Gaussians 
            whose means are randomly initialized and remaining parameters are 
            set constant. We are using an RTX 2070 Super with 8GB of video memory.
        </p>

        <h6>Performance</h6>
            <p>
                First we are evaluating the perfomance of the forward and 
                backward passes at various tile sizes:
            </p>

            <div class="row">
                <div class="seven column category" style="text-align: center;">
                    <h5 class="docs-header">Table 1</h5>
                    <table class="u-full-width">
                        <thead>
                            <tr>
                                <th>Tile Size</th>
                                <th>Mean Forward</th>
                                <th>Mean Backward</th>
                                <th>Mean Total</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>8</td>
                                <td>1.304s</td>
                                <td>1.357s</td>
                                <td>2.661s</td>
                            </tr>
                            <tr>
                                <td>16</td>
                                <td>0.503s</td>
                                <td>0.397s</td>
                                <td>0.9s</td>
                            </tr>
                            <tr>
                                <td><b>32</b></td>
                                <td><b>0.362s</b></td>
                                <td><b>0.472s</b></td>
                                <td><b>0.834s</b></td>
                            </tr>
                            <tr>
                                <td>64</td>
                                <td>0.471s</td>
                                <td>1.148s</td>
                                <td>1.619s</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <p>
                    We are measuring the best overall perfomance at a tile size of 32.
                </p>

            </div>
              
        <h6>Qualitative Results</h6>

        
    <!-- <div class="docs-section" id="code">
        <h3 class="section-heading">Code</h3>
        <p class="section-description">At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata ita sanctus est Lorem ipsum dolor sit amet.</p>
        <pre>
            <code>def load_image(nr, flat=True):
    img = io.imread(get_path(nr), as_grey=True)
    if flat:
        # print(img.shape)
        img = img.reshape(-1)
        # print(np.amin(img), np.amax(img))
    print("Loaded img " + str(nr))
    return img
</code>
        </pre>
        <p>
            Inline code snippets <code>are also possible</code> as this beautiful <code>super code i++; 0x34</code> shows.
        </p>
    </div>

    <h5>Images in columns</h5> -->

    <div class="docs-section" id="references">
        <h3 class="section-heading">References</h3>
        <ul class="popover-list">
            <li class="popover-item" id="1">
                [1] Kerbl, Bernhard, et al. "3D Gaussian Splatting for Real-Time Radiance Field Rendering." ACM Trans. Graph. 42.4 (2023): 139-1.
            </li>

            <li class="popover-item" id="2">
                [2] Westoby, Matthew J., et al. "‘Structure-from-Motion’photogrammetry: A low-cost, effective tool for geoscience applications." Geomorphology 179 (2012): 300-314.
            </li>

            <li class="popover-item" id="3">
                [3] Zwicker, Matthias, et al. "EWA splatting." IEEE Transactions on Visualization and Computer Graphics 8.3 (2002): 223-238.
            </li>

            <li class="popover-item" id="3">
                [4] Mildenhall, Ben, et al. "Nerf: Representing scenes as neural radiance fields for view synthesis." Communications of the ACM 65.1 (2021): 99-106.
            </li>
        </ul>
    </div>

</div>

